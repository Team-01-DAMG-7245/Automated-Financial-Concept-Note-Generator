The top panel (ISTWAP 0, T −ISAgent 0, T ) shows the final performance of the agent for the training
data as the differences in IS between TWAP and the agent at the ends of the horizons at times T. For
example, in the top panel, you can see the outperformance of the agent (positive IS difference) in the
first three horizons, while the agent underperforms in some of the other horizons. The middle panel
(ISTWAP 0, t −ISAgent 0, t ) shows the differences in IS for all time steps t, including the ones before
the ends of the horizons. The middle panel does not appear to track the top panel values very well in
some steps (except for the very last steps in the horizons, which are the top panel values when t = T),
as there are frequent trend reversals between negative and positive values at the last steps of the
horizons. Therefore, the simple difference in IS between TWAP and the agent at each step
(ISTWAP 0, t −ISAgent 0, t ) shown in the middle panel may not be a good reward function. On the
other hand, the bottom panel, which plots the rewards used by the agent in this example at each step,
appears to track the top panel values more closely than the middle panel at most steps.

Display Summary of Training Results for Sell Trades

Display the total Implementation Shortfall outperformance of the agent over TWAP for the training
data.

sum(IS_TWAP_Horizon_Train_Sell - IS_Agent_Horizon_Train_Sell)

ans = 
164.4700

Display total-gain-to-loss ratio (TGLR) for the training data.

ISTGLR(IS_TWAP_Horizon_Train_Sell - IS_Agent_Horizon_Train_Sell)

Deep Reinforcement Learning for Optimal Trade Execution

4-427